{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN7hPYrbpeiLTqweQjLP8Dd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "36a8f82c30f14cdbadf731a0a3dc08cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b5ebb50d8234169b3f4c47b1826d16d",
              "IPY_MODEL_0c026d3179f54000a1f8ac2901752f2a",
              "IPY_MODEL_28878880c8474a0c80bb8588f57958cf"
            ],
            "layout": "IPY_MODEL_9a358e3c174a4f6e82d583e14dc144b7"
          }
        },
        "1b5ebb50d8234169b3f4c47b1826d16d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95c9e8ff20704c01bc74d77d29d73bba",
            "placeholder": "​",
            "style": "IPY_MODEL_7d6c9f2b1f1641959aebf548fc95aafb",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "0c026d3179f54000a1f8ac2901752f2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c48c1e9c73bd425b831e1d5bfcaed45b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6dc6ec90383744658101b3c49da1079b",
            "value": 2
          }
        },
        "28878880c8474a0c80bb8588f57958cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b991f8f580fc4cc6aa080dc8144a37cd",
            "placeholder": "​",
            "style": "IPY_MODEL_c245df4a1c0e4f72984843ee12031dba",
            "value": " 2/2 [00:32&lt;00:00, 14.73s/it]"
          }
        },
        "9a358e3c174a4f6e82d583e14dc144b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95c9e8ff20704c01bc74d77d29d73bba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d6c9f2b1f1641959aebf548fc95aafb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c48c1e9c73bd425b831e1d5bfcaed45b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dc6ec90383744658101b3c49da1079b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b991f8f580fc4cc6aa080dc8144a37cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c245df4a1c0e4f72984843ee12031dba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gagan3012/EAIS-Temporal-Bias/blob/master/RQ3_Model_embedding_changes_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AQMVCTw5y2J4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import List, Dict\n",
        "from datetime import datetime\n",
        "from random import sample\n",
        "\n",
        "class TemporalBiasAnalyzer:\n",
        "    def __init__(self, model_name: str):\n",
        "        \"\"\"Initialize the analyzer with a specific model.\"\"\"\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name, output_hidden_states=True, device_map=\"auto\")\n",
        "        self.model.eval()\n",
        "        self.date_formats = [\n",
        "            '%Y%m%d', '%d%m%Y', '%m%d%Y',  # No separator\n",
        "            '%Y-%m-%d', '%d-%m-%Y', '%m-%d-%Y',  # Hyphen\n",
        "            '%Y/%m/%d', '%d/%m/%Y', '%m/%d/%Y',  # Slash\n",
        "            '%Y.%m.%d', '%d.%m.%Y', '%m.%d.%Y',  # Dot\n",
        "            '%Y %m %d', '%d %m %Y', '%m %d %Y',  # Space\n",
        "        ]\n",
        "        # self.date_formats = sample(self.date_formats, 7)  # Randomly sample 7 formats\n",
        "\n",
        "    def format_date(self, date_str: str, input_format: str, output_format: str) -> str:\n",
        "        \"\"\"Convert date from one format to another.\"\"\"\n",
        "        date_obj = datetime.strptime(date_str, input_format)\n",
        "        return date_obj.strftime(output_format)\n",
        "\n",
        "    def create_temporal_dataset(self) -> Dict[str, Dict[str, List[str]]]:\n",
        "        \"\"\"Create a dataset with different date formats and temporal contexts.\"\"\"\n",
        "        template_texts = [\n",
        "            \"By {date}, the most significant technological advancement was\",\n",
        "            \"As of {date}, people commonly used\",\n",
        "            \"On {date}, scientists discovered\",\n",
        "            \"In {date}, the most popular music genre was\",\n",
        "            \"By {date}, humans are expected to have colonized\",\n",
        "            \"On {date}, artificial intelligence will have\",\n",
        "            \"On {date}, the world population reached\",\n",
        "            \"By {date}, it is projected that the global population will be\",\n",
        "            \"In {date}, the average global temperature was\",\n",
        "            \"Just provide your final answer: The time 7 year and 9 month after {date} is\",\n",
        "             \"On {date}, researchers discovered quantum tunneling effects in superconductors, a major advancement from the prior decade.\",\n",
        "            \"AI achieved consciousness on {date}, marking a leap from prior discoveries in 2023.\",\n",
        "            \"On {date}, scientists reported an event similar to one that occurred exactly 25 years earlier.\",\n",
        "            \"On {date}, the first interstellar colony was established, occurring precisely 50 years after Apollo 11's mission.\",\n",
        "            \"Global climate solutions implemented on {date} were inspired by technologies developed decades earlier.\",\n",
        "            \"The first iPhone was released on {date}. How many years has it been since its release?\",\n",
        "            \"John was born on {date}. He graduated from college on 01-05-2007. Was John older than 18 when he graduated?\"\n",
        "            \"Which from the following famous people died on {date}? Answer only with A,B,C or D: A) Shah Jahan B) Miguel de Cervantes C) Princess Diana D) William Shakespeare\",\n",
        "            \"Who died on {date}?\",\n",
        "            \"How many years has it been since {date}?\",\n",
        "            \"How many years have passed since {date}?\",\n",
        "            \"How many years will it be since {date} on {date}?\",\n",
        "            \"How long has it been since {date}?\",\n",
        "            \"If pre-orders open 10 months earlier than {date}, on what date can customers start preordering?\",\n",
        "            \"When will a subscription service's next billing cycle take place if a user signs up on {date}, and the service charges every 10 days?\",\n",
        "            \"What will the contract's last day be if an individual is hired on {date} with the agreement lasting for 30 years?\",\n",
        "            \"Considering the Burj Khalifa's completion on {date}, how many full decades had passed before Alice was born on {date}?\",\n",
        "            \"Was Monty's planned event on {date} before their actual birthday on {date}?\",\n",
        "            \"If an event is scheduled 103 years from {date}, on what date will it occur?\",\n",
        "            \"What is the time 9 years and 3 months after {date}?\"\n",
        "            \"What is the time 2 years and 8 months before {date}?\"\n",
        "        ]\n",
        "\n",
        "        historical_dates = [\"19801015\", \"17950722\", \"16880305\", \"19720918\", \"19900430\"]\n",
        "        present_dates = [\"20241015\", \"20240722\", \"20230305\", \"20230918\", \"20220430\"]\n",
        "        future_dates = [\"20501015\", \"20650722\", \"20780305\", \"20820918\", \"20900430\"]\n",
        "\n",
        "        dataset = {'past': {}, 'present': {}, 'future': {}}\n",
        "\n",
        "        for date_format in self.date_formats:\n",
        "            dataset['past'][date_format] = []\n",
        "            dataset['present'][date_format] = []\n",
        "            dataset['future'][date_format] = []\n",
        "            for template, hist_date, pres_date, fut_date in zip(template_texts, historical_dates, present_dates, future_dates):\n",
        "                hist_formatted = self.format_date(hist_date, '%Y%m%d', date_format)\n",
        "                pres_formatted = self.format_date(pres_date, '%Y%m%d', date_format)\n",
        "                fut_formatted = self.format_date(fut_date, '%Y%m%d', date_format)\n",
        "                dataset['past'][date_format].append(template.format(date=hist_formatted))\n",
        "                dataset['present'][date_format].append(template.format(date=pres_formatted))\n",
        "                dataset['future'][date_format].append(template.format(date=fut_formatted))\n",
        "        return dataset\n",
        "\n",
        "    def get_model_outputs(self, texts: List[str]) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"Extract embeddings and logits for a list of texts.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n",
        "            outputs = self.model(**inputs)\n",
        "            hidden_states = outputs.hidden_states\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Average embeddings over last 4 layers and tokens\n",
        "            avg_embeddings = torch.stack(hidden_states[-4:]).mean(dim=0).mean(dim=1).cpu()\n",
        "            return {\n",
        "                \"embeddings\": avg_embeddings,\n",
        "                \"logits\": logits.cpu()\n",
        "            }\n",
        "\n",
        "    def analyze_dataset(self, dataset: Dict[str, Dict[str, List[str]]]) -> Dict[str, Dict]:\n",
        "        \"\"\"Analyze softmax outputs and embeddings across temporal references and date formats.\"\"\"\n",
        "        temporal_references = ['past', 'present', 'future']\n",
        "        analysis = {\"softmax\": {}, \"embeddings\": {}}\n",
        "\n",
        "        for temp_ref in temporal_references:\n",
        "            embeddings_per_format = {}\n",
        "            softmax_per_format = {}\n",
        "\n",
        "            for date_format, texts in dataset[temp_ref].items():\n",
        "                outputs = self.get_model_outputs(texts)\n",
        "                embeddings = outputs[\"embeddings\"]\n",
        "                logits = outputs[\"logits\"]\n",
        "\n",
        "                # Average embeddings and softmax probabilities\n",
        "                embeddings_per_format[date_format] = embeddings.mean(dim=0)\n",
        "                mean_logits = logits.mean(dim=0).mean(dim=0)\n",
        "                softmax_probs = torch.softmax(mean_logits, dim=0)\n",
        "                softmax_per_format[date_format] = softmax_probs\n",
        "\n",
        "            analysis[\"embeddings\"][temp_ref] = embeddings_per_format\n",
        "            analysis[\"softmax\"][temp_ref] = softmax_per_format\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    def visualize_biases(self, analysis: Dict[str, Dict], data_type: str, model_name: str):\n",
        "        \"\"\"Visualize biases for embeddings or softmax outputs in a single figure.\"\"\"\n",
        "        temporal_references = list(analysis[data_type].keys())\n",
        "\n",
        "        # Temporal reference-wise comparison\n",
        "        temp_ref_data = {ref: torch.stack(list(data.values())).mean(dim=0) for ref, data in analysis[data_type].items()}\n",
        "        temp_refs = list(temp_ref_data.keys())\n",
        "\n",
        "        # Compute similarity matrix for temporal references\n",
        "        num_refs = len(temp_refs)\n",
        "        temp_ref_similarity_matrix = np.zeros((num_refs, num_refs))\n",
        "\n",
        "        for i, ref_i in enumerate(temp_refs):\n",
        "            for j, ref_j in enumerate(temp_refs):\n",
        "                if data_type == \"embeddings\":\n",
        "                    temp_ref_similarity_matrix[i, j] = torch.nn.functional.cosine_similarity(\n",
        "                        temp_ref_data[ref_i].unsqueeze(0), temp_ref_data[ref_j].unsqueeze(0)\n",
        "                    ).item()\n",
        "                elif data_type == \"softmax\":\n",
        "                    p = temp_ref_data[ref_i].numpy() + 1e-10\n",
        "                    q = temp_ref_data[ref_j].numpy() + 1e-10\n",
        "                    temp_ref_similarity_matrix[i, j] = np.sum(p * np.log(p / q))\n",
        "\n",
        "        # Normalize for heatmap\n",
        "        temp_ref_similarity_matrix = (temp_ref_similarity_matrix - temp_ref_similarity_matrix.min()) / (\n",
        "            temp_ref_similarity_matrix.max() - temp_ref_similarity_matrix.min()\n",
        "        )\n",
        "\n",
        "        # Prepare figure with subplots\n",
        "        num_format_plots = len(temporal_references)\n",
        "        fig, axes = plt.subplots(1, num_format_plots+1, figsize=(24, 8))\n",
        "        if data_type == \"embeddings\":\n",
        "            fig.suptitle(f\"Representation-Level Temporal Bias Analysis\", fontsize=24)\n",
        "        elif data_type == \"softmax\":\n",
        "            fig.suptitle(f\"Logical-level Temporal bias Analysis\", fontsize=24)\n",
        "\n",
        "        # Plot temporal reference comparison heatmap (row 1, col 1)\n",
        "        sns.heatmap(\n",
        "            temp_ref_similarity_matrix,\n",
        "            annot=True,\n",
        "            fmt=\".2f\",\n",
        "            xticklabels=temp_refs,\n",
        "            yticklabels=temp_refs,\n",
        "            cmap=\"coolwarm\",\n",
        "            vmin=0,\n",
        "            vmax=1,\n",
        "            ax=axes[0],\n",
        "            cbar=False\n",
        "        )\n",
        "        axes[0].set_title(f\"Temporal Reference Comparison\", fontsize=16)\n",
        "        axes[0].set_xlabel(\"Temporal References\")\n",
        "        axes[0].set_ylabel(\"Temporal References\")\n",
        "\n",
        "        # Plot format-wise comparisons\n",
        "        for col_idx, temp_ref in enumerate(temporal_references):\n",
        "            data = analysis[data_type][temp_ref]\n",
        "            labels = list(data.keys())\n",
        "            num_labels = len(labels)\n",
        "\n",
        "            # Compute similarity matrix for date formats\n",
        "            format_similarity_matrix = np.zeros((num_labels, num_labels))\n",
        "            for i, key_i in enumerate(labels):\n",
        "                for j, key_j in enumerate(labels):\n",
        "                    if data_type == \"embeddings\":\n",
        "                        format_similarity_matrix[i, j] = torch.nn.functional.cosine_similarity(\n",
        "                            data[key_i].unsqueeze(0), data[key_j].unsqueeze(0)\n",
        "                        ).item()\n",
        "                    elif data_type == \"softmax\":\n",
        "                        p = data[key_i].numpy() + 1e-10\n",
        "                        q = data[key_j].numpy() + 1e-10\n",
        "                        format_similarity_matrix[i, j] = np.sum(p * np.log(p / q))\n",
        "\n",
        "            # Normalize for heatmap\n",
        "            format_similarity_matrix = (format_similarity_matrix - format_similarity_matrix.min()) / (\n",
        "                format_similarity_matrix.max() - format_similarity_matrix.min()\n",
        "            )\n",
        "\n",
        "            # Plot format comparison heatmap (row 1, columns 2-4)\n",
        "            #annot only the last one\n",
        "            sns.heatmap(\n",
        "                format_similarity_matrix,\n",
        "                annot=True,\n",
        "                fmt=\".2f\",\n",
        "                xticklabels=labels,\n",
        "                yticklabels=labels,\n",
        "                cmap=\"coolwarm\",\n",
        "                vmin=0,\n",
        "                vmax=1,\n",
        "                ax=axes[col_idx+1],\n",
        "                cbar=False\n",
        "            )\n",
        "            axes[col_idx+1].set_title(f\"{temp_ref.capitalize()} Date Formats\", fontsize=16)\n",
        "            axes[col_idx+1].set_xlabel(\"Date Formats\")\n",
        "            axes[col_idx+1].set_ylabel(\"Date Formats\")\n",
        "\n",
        "        # Adjust layout\n",
        "        plt.tight_layout()\n",
        "        plt.subplots_adjust(top=0.9)\n",
        "        model_name = model_name.replace(\"/\", \"_\")\n",
        "        fig.savefig(f\"{model_name}_{data_type}.png\", dpi=300, bbox_inches=\"tight\")  # High resolution and tight layout\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def main(model_name):\n",
        "    print(f\"Analyzing model: {model_name}\")\n",
        "    analyzer = TemporalBiasAnalyzer(model_name)\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = analyzer.create_temporal_dataset()\n",
        "\n",
        "    # Analyze dataset\n",
        "    analysis = analyzer.analyze_dataset(dataset)\n",
        "\n",
        "    # Visualize biases\n",
        "    for data_type in [\"embeddings\", \"softmax\"]:\n",
        "        analyzer.visualize_biases(analysis, data_type, model_name)\n",
        "\n",
        "    print(f\"Analysis complete for model: {model_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_names = [\n",
        "        # \"HuggingFaceTB/SmolLM2-360M-Instruct\",\n",
        "        # \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "        \"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "        # \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "    ]\n",
        "for model_name in model_names:\n",
        "    main(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "36a8f82c30f14cdbadf731a0a3dc08cf",
            "1b5ebb50d8234169b3f4c47b1826d16d",
            "0c026d3179f54000a1f8ac2901752f2a",
            "28878880c8474a0c80bb8588f57958cf",
            "9a358e3c174a4f6e82d583e14dc144b7",
            "95c9e8ff20704c01bc74d77d29d73bba",
            "7d6c9f2b1f1641959aebf548fc95aafb",
            "c48c1e9c73bd425b831e1d5bfcaed45b",
            "6dc6ec90383744658101b3c49da1079b",
            "b991f8f580fc4cc6aa080dc8144a37cd",
            "c245df4a1c0e4f72984843ee12031dba"
          ]
        },
        "id": "vvefHd7dy24U",
        "outputId": "7530ec6e-daeb-4f66-a64e-ca60dcbae0a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing model: meta-llama/Llama-3.2-3B-Instruct\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36a8f82c30f14cdbadf731a0a3dc08cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kvdYxCJuy-ob"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}